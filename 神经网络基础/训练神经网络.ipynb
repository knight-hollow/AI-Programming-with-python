{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9505d29e",
   "metadata": {},
   "source": [
    "# 神经网络\n",
    "## 神经网络简介\n",
    "### 训练神经网络\n",
    " \n",
    "训练集(Training Set): 用于训练模型的数据，即让模型“学习”的数据。  \n",
    "测试集(Test Set): 独立于训练的数据集，用来评估模型的泛化能力。\n",
    "\n",
    "欠拟合(Underfitting): 模型过于简单，无法捕捉数据中的规律. 训练集误差高，测试集误差也高。   \n",
    "原因: 模型过于简单/特征不足，信息量不够/训练不足\n",
    "\n",
    "过拟合(Overfitting): 模型过于复杂，把训练集“背”下来了，甚至学习了噪声，导致泛化能力差。训练集误差很低，但测试集误差很高。  \n",
    "原因: 模型过于复杂/训练集太少/训练时间太长\n",
    "\n",
    "如何避免过拟合：\n",
    "#### Early Stopping \n",
    "就是在验证误差最小的时候停止训练，避免模型在训练集上过度拟合。\n",
    "\n",
    "#### L1和L2正则化\n",
    "对于sigmoid函数，当权重较小时，函数的斜率比较适合梯度下降（曲线平滑，梯度容易计算）。当权重过大，结果更接近0和1，函数变得非常陡峭，梯度下降会变得很困难。对应了神经网络训练中的梯度消失问题\n",
    "权重过大而导致的过拟合优一下解决方法：\n",
    "1. L1 正则化 (L1 Regularization)\n",
    "\n",
    "目标函数：\n",
    "$\n",
    "L_{\\text{total}} = L_{\\text{original}} + \\lambda \\sum_{i} |w_i|\n",
    "$\n",
    "\n",
    "- $\\lambda$：正则化强度\n",
    "- $w_i$：模型参数\n",
    "\n",
    "特点\n",
    "- 会让一些权重变为 0 → **稀疏解**\n",
    "- 起到 **特征选择** 的作用\n",
    "- 也叫 **Lasso Regression**\n",
    "\n",
    "2. L2 正则化 (L2 Regularization)\n",
    "\n",
    "目标函数：\n",
    "$\n",
    "L_{\\text{total}} = L_{\\text{original}} + \\lambda \\sum_{i} w_i^2\n",
    "$\n",
    "\n",
    "- $\\lambda$：正则化强度\n",
    "- $w_i$：模型参数\n",
    "\n",
    "特点\n",
    "- 抑制权重过大，防止过拟合\n",
    "- 权重趋向于小值，但通常不为 0\n",
    "- 又叫 **岭回归 (Ridge Regression)**\n",
    "\n",
    "L1：会产生稀疏解（部分参数变为 0） → 特征选择.  \n",
    "L2：权重收缩但不为 0 → 更适合防止过拟合\n",
    "\n",
    "#### 随机失活(dropout)\n",
    "是一种防止神经网络过拟合的正则化方法。\n",
    "\n",
    "在训练神经网络时，有时网络的某个部分权重很大，最终主导了训练，而网络的另一部分实际上并没有发挥太大作用（因此得不到训练）。为了解决这个问题，我们可以使用一种称为 dropout 的方法，即关闭网络的一部分，让其余部分继续训练。每一轮迭代里，随机“丢弃”(置零) 一部分神经元（包括它们的输出和连接权重）。这会迫使其他节点弥补不足，并在训练中承担更大的责任。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1eaa60",
   "metadata": {},
   "source": [
    "在神经网络训练时，损失函数通常是高维、非凸的：这就会导致很多局部最小点，优化算法（比如梯度下降）可能“卡在”局部最小，而没到达全局最小。为了解决这个问题，我们采用随机重启(random restart):从不同的随机初始点开始运行优化算法,每次可能收敛到不同的局部最小值。最后从这些解中选择一个最优的（通常是损失函数值最小的）。\n",
    "- 批量梯度下降(batch gradient descent)\n",
    "    - 每次更新参数时，用整个训练集 计算损失函数的梯度。\n",
    "    - 参数更新方向：损失函数的全局梯度方向。\n",
    "    - 计算开销大（尤其是大数据集）\n",
    "    - 每次迭代都要遍历整个数据集，训练慢\n",
    "    - 不适合大规模深度学习\n",
    "- 随机梯度下降(stochastic gradient descent)\n",
    "    - 每次更新只用一个样本 → 计算效率高\n",
    "\t- 更新带有噪声，可以帮助跳出局部最小值\n",
    "    - 更新方向不稳定，损失函数曲线会“抖动”\n",
    "\t- 可能收敛较慢\n",
    "\n",
    "学习率 $\\eta$ 控制每次参数更新的步长\n",
    "- 学习率太大\n",
    "    - 每次走太大步，可能越过最优解\n",
    "    - 甚至导致训练发散，不收敛\n",
    "- 学习率太小\n",
    "    - 每次走很小步，训练速度非常慢\n",
    "    - 容易陷入局部最小，难以跳出\n",
    "- 合适的学习率\n",
    "    - 更新既快又稳定\n",
    "    - 能够收敛到较优解\n",
    "\n",
    "动量法(Momentum)\n",
    "引入了“惯性”的概念，让参数更新时考虑之前的更新方向，从而加速收敛并减少震荡。\n",
    "\n",
    "普通梯度下降：\n",
    "$\n",
    "\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_\\theta L(\\theta)\n",
    "$\n",
    "\n",
    "Momentum 更新公式：\n",
    "1. 速度更新：\n",
    "$\n",
    "v_t = \\beta v_{t-1} + (1-\\beta)\\nabla_\\theta L(\\theta)\n",
    "$\n",
    "2. 参数更新：\n",
    "$\n",
    "\\theta \\leftarrow \\theta - \\eta v_t\n",
    "$\n",
    "\n",
    "- $\\beta$ = 动量系数 (常用 0.9)\n",
    "- $v_t$ = 带惯性的累计梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad969e4",
   "metadata": {},
   "source": [
    "梯度消失(Vanishing gradient)：\n",
    "- Sigmoid/Tanh 激活函数在输入很大或很小时，导数接近 0, 造成梯度不断缩小\n",
    "- 深层网络梯度逐层相乘，容易趋近于 0\n",
    "- 权重初始化不当:如果初始值太小，也会导致梯度传播困难\n",
    "\n",
    "采用其他激活方程(ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc96388c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3h/vb9khfs12bjf5dykzt41gqp80000gn/T/ipykernel_4031/3151425541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helper'"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
