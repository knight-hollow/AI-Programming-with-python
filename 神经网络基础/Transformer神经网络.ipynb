{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de05d8f3",
   "metadata": {},
   "source": [
    "# Transformer神经网络\n",
    "## 自然语言处理NLP(Natural Language Processing)\n",
    "究计算机如何理解和处理人类语言的领域。\n",
    "\n",
    "### 标记化(Tokenization)\n",
    "在 NLP 中，原始文本（字符串）必须先转换成 计算机能处理的数值表示。这一步叫 Tokenization.  \n",
    "为了将单词一致地转换为 token ID，我们需要一个词汇表——一个包含大量文本集合中所有不同单词的列表。每个唯一单词都会获得一个唯一的 ID。  \n",
    "词汇表创建流程：\n",
    "- 文本收集：收集大量文本\n",
    "- 拆分：将文本拆分成单个单词。\n",
    "- 分配 ID：每个唯一单词都会获得一个唯一的 token ID。\n",
    "然后，该词汇表将用于将新的输入文本转换为一系列 ID\n",
    "\n",
    "标记化方法：\n",
    "- 按词分割 (Word-level Tokenization)    \n",
    "    - 将文本拆分成单词。\n",
    "    - 每个单词都会收到一个分词 ID。\n",
    "    - 优点：更多有意义的分词。\n",
    "    - 缺点：词汇量更大，处理未知单词困难。\n",
    "- 按字符分割 (Character-level Tokenization)\n",
    "    - 将文本拆分成单个字符。\n",
    "    - 每个字符都会收到一个分词 ID。\n",
    "    - 优点：可以轻松处理新单词。\n",
    "    - 缺点：单个分词意义不大。\n",
    "- 子词分割 (Subword Tokenization, 最常用)\n",
    "    - 合并完整单词和词块。\n",
    "    - 通过将未知单词分解成已知的子词来帮助处理它们。\n",
    "    - 优点：常见词保持完整、生僻词拆分成子词组合、词表大小适中\n",
    "\n",
    "#### The Bag of Words Model\n",
    "工作原理：\n",
    "- 词袋模型使用二进制向量作为输入。\n",
    "- ​​向量中的每个值都是 1 或 0，由输入标记 ID 创建。  \n",
    "\n",
    "步骤：\n",
    "- Tokenization\n",
    "    - 从输入句子开始，将其拆分成单个分词。\n",
    "    - 将所有单词转换为小写。\n",
    "    - 删除那些几乎没有有用信息的单词，例如“a”、“the”和其他停用词。\n",
    "    - 将每个分词转换为相应的分词 ID。\n",
    "- 创建输入向量：\n",
    "    - 对于每个分词 ID，例如，如果 ID 值为 345，则在向量中的相应位置设置一个分词 ID。\n",
    "    - 对所有其他输入 ID 重复此过程。\n",
    "    - 输入向量的元素数量与词汇表中唯一单词的数量相同。例如，如果词汇表包含 65,000 个单词，则输入向量将包含 65,000 个元素。\n",
    "\n",
    "模型训练与应用\n",
    "- 利用这个二元向量，我们可以将输入传递给神经网络或其他机器学习模型。\n",
    "- 使用这些输入向量训练模型，可以使其执行诸如文本分类之类的任务。\n",
    "\n",
    "局限性：\n",
    "- 忽略词序：该模型不考虑单词出现的顺序，而这对于理解上下文至关重要。\n",
    "- 忽略语法结构：它忽略标点符号和句子结构，错失了重要的语言细微差别。\n",
    "- 语义含义有限：该模型的标记表示仅指示单词的存在，缺乏更深层次的语义洞察。\n",
    "- 处理未见单词：该模型难以处理训练期间未见的单词，因为它们未包含在词汇表中。\n",
    "\n",
    "### 嵌入表示(Embedding)\n",
    "Embedding vector: 每个单词都表示为一个较小的多维向量，其中每个值代表与模型相关的单词的某些特征。  \n",
    "向量的大小在训练之前就决定好了，模型在训练过程中学习这些向量。  \n",
    "\n",
    "Embedding vector的特性:  \n",
    "- 可解释性：嵌入向量中的具体值无法直接解释，但它们具有有意义的属性。\n",
    "- 维度：嵌入向量通常具有数百或数千个维度，因此难以可视化。\n",
    "- 语义相似度：含义相似的词语由相似的向量表示。\n",
    "\n",
    "Word Embeddings的方法\n",
    "- 预训练嵌入模型：这些是专门为计算词嵌入而训练的独立模型，可用于各种应用。\n",
    "- Transformer 和嵌入：在 Transformer 模型的训练过程中，词嵌入会作为学习过程的一部分进行学习，因此无需单独的嵌入库。\n",
    "\n",
    "#### 循环神经网络RNN(Recurrent Neural Network)\n",
    "RNN 用于处理序列数据（文本、语音、时间序列），它最大的特点是：能够记住历史信息（有记忆能力）。和普通神经网络不同，RNN 在计算时会把 前一步的隐状态带入下一步，和当前输入一起计算输出。\n",
    "\n",
    "RNN 顺序数据处理\n",
    "\n",
    "1. **逐个 Token 处理**  \n",
    "   - 输入序列按顺序进入 RNN  \n",
    "   - 当前 token + 上一步隐藏状态 → 当前输出\n",
    "\n",
    "2. **隐藏状态 (Hidden State)**  \n",
    "   - 保存之前 token 的信息  \n",
    "   - 公式：\n",
    "   $\n",
    "   h_t = f(W_x x_t + W_h h_{t-1} + b)\n",
    "   $\n",
    "   $\n",
    "   y_t = W_y h_t + c\n",
    "   $\n",
    "\n",
    "3. **初始状态 (Initial State)**  \n",
    "   - 第一个 token 使用初始隐藏状态 $h_0$（通常是零向量）\n",
    "   - 隐藏状态贯穿整个序列\n",
    "\n",
    "RNN 模式.  \n",
    "- 标准 RNN\n",
    "    - **One-to-One**\n",
    "        - 输入序列 → 最终输出\n",
    "        - 用于分类任务（如垃圾邮件检测）\n",
    "\n",
    "- 变体\n",
    "1. **One-to-Many**\n",
    "   - 一个输入 → 多个输出\n",
    "   - 应用：图像描述生成、音乐生成\n",
    "\n",
    "2. **Many-to-Many**\n",
    "   - 输入序列 → 输出序列\n",
    "   - 应用：\n",
    "     - 序列标注（输入输出等长，如词性标注）\n",
    "     - 机器翻译（输入输出长度可不同）\n",
    "\n",
    "RNN 面临的挑战  \n",
    "- 难以访问先前状态：由于隐藏状态更新过程的性质，RNN 难以保留很久以前见过的标记信息。\n",
    "- 梯度递减问题：在深度网络中，梯度可能会变得非常小，从而减慢训练速度，并使其难以捕捉长序列的依赖关系。\n",
    "\n",
    "### Transformer\n",
    "注意力机制(attentation Mechanism): 嵌入向量之间的交互过程称为注意机制，它使用注意分数来显示输入标记之间的关系。   \n",
    "Transformer 模块：由一个注意模块和一个前馈网络组成。  \n",
    "多个 Transformer 模块（层）链接在一起，一个层的输出成为下一个层的输入。  \n",
    "\n",
    "Transformer 的优势\n",
    "- 并行处理：与 RNN 不同，Transformer 可以同时处理整个输入提示。\n",
    "- 长文本处理效率：能够更好地处理较长的文本。\n",
    "- 训练便捷：与 RNN 模型相比，Transformer 的训练过程更直接。\n",
    "- 性能：在实际任务中表现出色。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
